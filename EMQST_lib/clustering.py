import numpy as np
from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as sch
import copy
from EMQST_lib import overlapping_tomography as ot

def fcluster_to_labels(fcluster_labels):
    """
    Converts flat cluster labels into overlapping cluster labels.
    This function takes a list of cluster labels generated by the `fcluster` function
    and converts it into a list of lists, where each sublist contains the indices of 
    the elements that belong to the same cluster. It also removes any empty clusters.
    Parameters:
    fcluster_labels (list of int): A list of cluster labels where each element represents 
                                the cluster assignment of the corresponding data point.
    Returns:
    list of list of int: A list of clusters, where each cluster is represented as a list 
                        of indices of the data points that belong to that cluster.
    """
    cluster = [[] for _ in range(max(fcluster_labels))]
    for i in range(len(fcluster_labels)):
        cluster[fcluster_labels[i]-1].append(i)
        
    # If some groups are empty
    while [] in cluster:
        cluster.remove([])
    return cluster


def split_large_clusters(dist_matrix, cluster_assignments, max_size=4):
    """
    Splits clusters that exceed a given size limit using k-means clustering.

    Parameters:
    - data: np.ndarray, the dataset.
    - cluster_assignments: np.ndarray, initial cluster labels from `fcluster`.
    - max_size: int, maximum allowed cluster size.

    Returns:
    - updated_assignments: np.ndarray, updated cluster labels.
    """
    # Create a dictionary to hold indices of points for each cluster
    clusters = {}
    for idx, cluster_label in enumerate(cluster_assignments):
        clusters.setdefault(cluster_label, []).append(idx)
    
    # Initialize a dictionary for new assignments
    updated_assignments = cluster_assignments.copy()
    next_label = max(cluster_assignments) + 1  # Next cluster label

    for cluster_label, indices in clusters.items():
        # Check the size of the current cluster
        if len(indices) > max_size:
            # Extract data for this cluster
            cluster_data = dist_matrix[indices]
            
            # Apply k-means clustering to split the cluster
            n_splits = len(indices) // max_size + 1  # Determine number of splits
            kmeans = KMeans(n_clusters=n_splits, random_state=42)
            kmeans.fit(cluster_data)
            labels = kmeans.labels_
            
            # Reassign points to new sub-clusters
            for i, idx in enumerate(indices):
                updated_assignments[idx] = next_label + labels[i]
            
            # Increment the next available label
            next_label += n_splits
        else:
            # Keep the cluster as is
            continue
    return updated_assignments


def ward_clustering(dist_matrix, threshold = None):
    """
    Implements Ward's hierarchical clustering algorithm.
    """
    
    if threshold is None:
        threshold = 1 # If no threadhold is spesified, we select the upper limit.
    
    np.fill_diagonal(dist_matrix, 0)  # Diagonal should be 0
    condensed_distance_matrix = sch.distance.squareform(dist_matrix)
    Z = sch.linkage(condensed_distance_matrix, method='ward')
    cluster_labels = sch.fcluster(Z, t=threshold, criterion='distance')
    return cluster_labels, Z # Returns Z for plotting


def create_distance_matrix_from_corr(corr_array, unique_corr_labels, n_qubits):
    corr_matrix = np.zeros((n_qubits,n_qubits))
    it = 0
    for i,j in unique_corr_labels:
        corr_matrix[i,j] = corr_array[it] 
        corr_matrix[j,i] = corr_array[it] 
        it += 1
    return 1 - corr_matrix

def are_sublists_equal(list1, list2):
    """
    Checks if every sublist in list1 exists in list2 and vice versa.

    Args:
        list1 (list of lists): The first list of lists.
        list2 (list of lists): The second list of lists.

    Returns:
        bool: True if both lists contain the same sublists, otherwise False.
    """
    # Convert sublists to tuples for comparison (tuples are hashable)
    set1 = {tuple(sublist) for sublist in list1}
    set2 = {tuple(sublist) for sublist in list2}
    
    return set1 == set2



def assign_init_cluster(cluster_correlator_array,corr_labels,n_qubits):
    """
    Creates initial cluster by sorting for highest correlation coefficients, 
    and grouping those qubit pairs together.
    
    """
    partitions = []
    it = 1
    while len(partitions) < n_qubits/2:
        index = np.argpartition(cluster_correlator_array, -(it))[-(it):][0]
        if not np.any(np.isin(corr_labels[index],partitions )):
            partitions.append(corr_labels[index].tolist())
            # print(f'Correlation strenght: {cluster_correlator_array[index]}')
            # print(f'Current partition: {partitions}')
        it+=1
    return partitions



def find_noise_cluster_structure(QDT_outcomes, n_qubits, n_QDT_shots, hash_family, n_hash_symbols, one_qubit_calibration_states, n_cores):
    print(f'Create all possible 2 qubit POVMs for correlation map.')
    two_point_POVM, corr_subsystem_labels = ot.reconstruct_all_two_qubit_POVMs(QDT_outcomes, n_qubits, hash_family, n_hash_symbols, one_qubit_calibration_states, n_cores)


    summed_quantum_corr_array, unique_corr_labels = ot.compute_quantum_correlation_coefficients(two_point_POVM, corr_subsystem_labels)

    # Find the full cluster size of the system

    corr_limit = 1/np.sqrt(n_QDT_shots)

    n_runs  = 5
    alpha_array = np.array([0 ,0.4, 0.5, 0.6]) 

    # Create initialpartition
    reward = -10**6
    for alpha in alpha_array:
        print(f'Alpha: {alpha}')
        two_point_init_cluster = assign_init_cluster(summed_quantum_corr_array,unique_corr_labels,n_qubits,corr_limit)
        one_point_init_cluster = [[i] for i in range(n_qubits)]
    # Optimize cluster structure according to heuristic cost function. 
        two_noise_cluster_labels_temp, two_reward_temp = optimize_cluster(n_runs,two_point_init_cluster,summed_quantum_corr_array,unique_corr_labels,4,corr_limit,alpha)
        one_noise_cluster_labels_temp, one_reward_temp = optimize_cluster(n_runs,one_point_init_cluster,summed_quantum_corr_array,unique_corr_labels,4,corr_limit,alpha)
        print(f'Found cluster structure\n {two_noise_cluster_labels_temp}, \n{one_noise_cluster_labels_temp}')
        print(f'Found reward {two_reward_temp}, {one_reward_temp}')
        if reward < two_reward_temp:
            noise_cluster_labels = two_noise_cluster_labels_temp
            reward = two_reward_temp
            print(f'New reward: {reward}')
            print(f'New cluster labels: {noise_cluster_labels}')
        if reward < one_reward_temp:
            noise_cluster_labels = one_noise_cluster_labels_temp
            reward = one_reward_temp
            print(f'New reward: {reward}')
            print(f'New cluster labels: {noise_cluster_labels}')
    return noise_cluster_labels


def optimize_cluster(n_runs,init_partition,corr_array,corr_labels,max_cluster_size, expected_large_values, alpha ):
    """
    Cluster optimization loop.
    """
    #print('Starting optimization of premade cluster structure.')
    rng = np.random.default_rng()
    best_partition = copy.deepcopy(init_partition)
    reward = obj_func(best_partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
    repetitions = 3
    for _ in range(repetitions):
        partition = copy.deepcopy(init_partition)
        for i in range(n_runs):
            #print('Run:',i)
            S_pairs = copy.deepcopy(corr_labels)
            S_pairs = rng.permutation(S_pairs)
            cost_0 = obj_func(partition, corr_array, corr_labels,max_cluster_size, expected_large_values, alpha)
            for pair in S_pairs:
                #print(pair,parition,np.isin(partitions,pair))
                if is_pair_in_more_than_one_cluster(pair,partition): # If pairs exist in more than one cluster:
                    
                    masked_partition = []  # Retrieve the partitions that include the pair
                    new_partition_1 = copy.deepcopy(partition)
                    temp_count = 0
                    for i in range(len(partition)): # Create a partition with the pair removed and one with just the pair
                        if np.any(np.isin(partition[i],pair)):
                            masked_partition.append(partition[i])
                            new_partition_1.pop(i-temp_count)
                            temp_count+=1
                    #print(f'Partion removed:{new_partition_1}')
                    #print(f'Parition added {masked_partition}')
                    #partion_mask = np.any(np.isin(parition,pair),axis=1) # Create mask for where what clusters include any of element in the pair

                    if len(masked_partition)>2:
                        print("Pair is assigned to more than 2 clusters.")
                        print(masked_partition)
                        return 0
                    # We now check 3 instances of these partitions:
                    # 1) Swap 1st qubit to second
                    # 2) Swat 2nd qubit to first
                    # 3) Exchange qubits between the two partitions
                    
                    new_partition_2 = copy.deepcopy(new_partition_1)
                    new_partition_3 = copy.deepcopy(new_partition_1)
                    #print(pair)
                    # 1) Swap 1st qubit to second
                    masked_partition_1 = copy.deepcopy(masked_partition)
                    #print(f'Original: {new_partition_1}, {masked_partition}, {pair}')
                    if pair[0] in masked_partition_1[0]:
                        masked_partition_1[0].remove(pair[0])
                        masked_partition_1[1].append(pair[0])
                    else:
                        masked_partition_1[1].remove(pair[0])
                        masked_partition_1[0].append(pair[0])
                    new_partition_1.append(masked_partition_1[0])
                    new_partition_1.append(masked_partition_1[1])

                    # 2) Swap 2nd qubit to first
                    masked_partition_2 = copy.deepcopy(masked_partition)
                    if pair[1] in masked_partition_2[0]:
                        masked_partition_2[0].remove(pair[1])
                        masked_partition_2[1].append(pair[1])
                    else:
                        masked_partition_2[1].remove(pair[1])
                        masked_partition_2[0].append(pair[1])
                    new_partition_2.append(masked_partition_2[0])
                    new_partition_2.append(masked_partition_2[1])
                
                    # 3) Exchange qubits between the two partitions
                    masked_partition_3 = copy.deepcopy(masked_partition)
                    if pair[0] in masked_partition_3[0]:
                        masked_partition_3[0].remove(pair[0])
                        masked_partition_3[1].append(pair[0])
                        masked_partition_3[1].remove(pair[1])
                        masked_partition_3[0].append(pair[1])
                    else:
                        masked_partition_3[1].remove(pair[0])
                        masked_partition_3[0].append(pair[0])
                        masked_partition_3[0].remove(pair[1])
                        masked_partition_3[1].append(pair[1])
                    new_partition_3.append(masked_partition_3[0])
                    new_partition_3.append(masked_partition_3[1])
                    #print(new_partition_1)
                    for new_partition in [new_partition_1,new_partition_2,new_partition_3]:
                        cost = obj_func(new_partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
                        #print(cost)
                        if cost > cost_0:
                            #print(f'New partition {new_partition}')
                            #print('Cost:',cost)
                            partition = copy.deepcopy(new_partition)
                            cost_0 = cost
                            
                            
                else: # If pair is in the same cluster, try to remove one from the cluster. 
                    masked_partition = []  # Retrieve the partitions that include the pair
                    new_partition_1 = copy.deepcopy(partition)
                    temp_count = 0
                    for o in range(len(partition)): # Create a partition with the pair removed and one with just the pair
                        if np.any(np.isin(partition[o],pair)):
                            masked_partition.append(partition[o])
                            new_partition_1.pop(o-temp_count)
                            temp_count+=1
                    # We remove one qubit into a new partition, then the other qubit into a new partition.
                    
                    new_partition_2 = copy.deepcopy(new_partition_1)
                    new_partition_3 = copy.deepcopy(new_partition_1)
                    masked_partition_1 = copy.deepcopy(masked_partition)
                    masked_partition_2 = copy.deepcopy(masked_partition)
                    masked_partition_3 = copy.deepcopy(masked_partition)
                    # Put first qubit into a new partition
                    masked_partition_1[0].remove(pair[0])
                    new_partition_1.append(masked_partition_1[0])
                    new_partition_1.append([pair[0]])
                    
                    # Put second qubit into new partition
                    masked_partition_2[0].remove(pair[1])
                    new_partition_2.append(masked_partition_2[0])
                    new_partition_2.append([pair[1]])
                    
                    # Remove both
                    masked_partition_3[0].remove(pair[1])
                    masked_partition_3[0].remove(pair[0])
                    new_partition_3.append(masked_partition_3[0])
                    new_partition_3.append([pair[1]])
                    new_partition_3.append([pair[0]])
                    
                    for new_partition in [new_partition_1,new_partition_2, new_partition_3]:
                        cost = obj_func(new_partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
                        if cost > cost_0:
                            #print(f'New partition {new_partition}')
                            #print('Cost:',cost)
                            #print('Created a new parition!')
                            partition = copy.deepcopy(new_partition)
                            cost_0 = cost
                
        if obj_func(partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha) > reward:
            reward = obj_func(partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
            best_partition = copy.deepcopy(partition)
    while [] in best_partition: # Remove empty partitions before sending back
        best_partition.remove([])
    best_partition = [np.sort(part)[::-1] for part in best_partition]           
    return best_partition, reward


def obj_func(partitions,corr_array,corr_labels, max_cluster_size, expected_large_values, alpha):
    """
    Objective for the cluster opitmization problem.
    Alpha: A tuning paramter, which tunes the penalty for large clusters. Large alpha discuourages large clusters.
    """
    cost = 0
    # Calculate the current cluster strenght

    S = np.zeros(len(partitions))
    for i in range(len(partitions)):
        mask = np.all(np.isin(corr_labels,partitions[i]),axis=1)
        #print(mask,partitions[i])
        S[i] = np.sum(corr_array[mask])
    S_sum = np.sum(S)
    partition_size = np.array([len(partition) for partition in partitions])
    
    # Finding c_avg
    large_corr = np.sort(np.abs(corr_array))[-expected_large_values:]
    c_avg = np.mean(large_corr)
    #print(f'New avg: {c_avg}')
    for i in range(len(partitions)):
        if partition_size[i] > max_cluster_size:
            cost -= 1e10
        else:
            cost -=c_avg*alpha*partition_size[i]**2
    return cost + S_sum


def is_pair_in_more_than_one_cluster(pair_label, clusters):
    """
    Check if a pair is in more than one cluster.
    Takes in a pair of qubits, and check the list of clusters if the pair is present in more than one cluster.
    """
    counter = 0
    for cluster in clusters:
        if np.any(np.isin(pair_label,cluster)):
            counter +=1
    if counter > 1:
        return True
    else:    
        return False

def get_true_cluster_labels(cluster_size):
    "Takes in cluster size_size and returns the qubit labels for the true states clustered together."
    true_cluster_labels = []
    it = 0
    rev_cluster_size = cluster_size[::-1]
    for i in range(len(cluster_size)):
        true_cluster_labels.append(np.arange(it, it + rev_cluster_size[i]))
        it+=rev_cluster_size[i]
    true_cluster_labels = [np.sort(cluster)[::-1] for cluster in true_cluster_labels]
    return true_cluster_labels[::-1]


def find_clusters_from_correlator_labels(correlator_labels, clusters):
    """
    From the correlation labels finds the clusters that contain any of the labels.

    Parameters:
    - correlator_labels (list): A list of correlation labels.
    - clusters (list): A list of clusters.

    Returns:
    - return_cluster (list): A list of clusters that contain any of the correlator labels.
    """
    return_cluster = []
    for label in correlator_labels:
        temp_cluster = []
        for cluster in clusters:
            if np.any(np.isin(cluster, label)): # label is in cluster
                temp_cluster.append(cluster) # Adds cluster to label
            
        return_cluster.append(temp_cluster)
    return return_cluster